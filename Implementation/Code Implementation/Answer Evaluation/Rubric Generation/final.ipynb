{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7928f8",
   "metadata": {},
   "source": [
    "## Technical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de284fd",
   "metadata": {},
   "source": [
    "### Make the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920434d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gasse\\AppData\\Local\\Temp\\ipykernel_17236\\2823508895.py:30: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n",
      "C:\\Users\\gasse\\AppData\\Local\\Temp\\ipykernel_17236\\2823508895.py:55: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\gasse\\AppData\\Local\\Temp\\ipykernel_17236\\2823508895.py:56: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  raw = chain.run(context=context)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rubric saved to rubric.json\n",
      "{\n",
      "  \"scores\": [\n",
      "    {\n",
      "      \"name\": \"Clarity\",\n",
      "      \"score\": 85,\n",
      "      \"explanation\": \"The answer is generally clear but uses technical terms that might not be accessible to all readers.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Accuracy\",\n",
      "      \"score\": 90,\n",
      "      \"explanation\": \"The methods mentioned are accurate for optimizing loops in Python but lack specific examples.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Completeness\",\n",
      "      \"score\": 75,\n",
      "      \"explanation\": \"The answer mentions key optimization methods but doesn't cover all possible approaches.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Relevance\",\n",
      "      \"score\": 95,\n",
      "      \"explanation\": \"The answer directly addresses the question by providing relevant techniques.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Depth\",\n",
      "      \"score\": 70,\n",
      "      \"explanation\": \"The answer is somewhat superficial and doesn't delve deeply into any of the methods.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Conciseness\",\n",
      "      \"score\": 90,\n",
      "      \"explanation\": \"The answer is succinct and avoids unnecessary verbosity.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Engagement\",\n",
      "      \"score\": 65,\n",
      "      \"explanation\": \"The answer is informative but lacks engaging elements to maintain interest.\"\n",
      "    }\n",
      "  ],\n",
      "  \"overall_score\": 81.42857142857143\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helper: robust JSON extraction ─────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    for start_idx, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_char, close_char = (ch, \"}\" if ch==\"{\" else \"]\")\n",
    "            balance = 0\n",
    "            for end_idx in range(start_idx, len(text)):\n",
    "                if text[end_idx] == open_char: balance += 1\n",
    "                elif text[end_idx] == close_char:\n",
    "                    balance -= 1\n",
    "                    if balance == 0:\n",
    "                        return json.loads(text[start_idx:end_idx+1])\n",
    "    raise ValueError(f\"No complete JSON object/array found in:\\n{text}\")\n",
    "\n",
    "\n",
    "# ─── 1. Load credentials ───────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"Azure\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "\n",
    "\n",
    "# ─── 2. Rubric generation ─────────────────────────────────────────────────────\n",
    "def generate_rubric(csv_path: str, max_examples: int = 50) -> list:\n",
    "    df = pd.read_csv(csv_path).dropna(subset=[\"question\", \"answer\"])\n",
    "    samples = df.head(max_examples)\n",
    "    context = \"\\n\\n\".join(f\"Q: {q}\\nA: {a}\" for q, a in zip(samples[\"question\"], samples[\"answer\"]))\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"\n",
    "You are an expert evaluator. Here are sample Q/A pairs:\n",
    "{context}\n",
    "\n",
    "Generate a JSON **array** of 5–8 evaluation criteria. Each criterion must be an object:\n",
    "- \"name\": short title\n",
    "- \"description\": one-sentence explanation\n",
    "\n",
    "**Only output the JSON array**, without any extra text.\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(context=context)\n",
    "    return extract_json(raw)\n",
    "\n",
    "\n",
    "# ─── 3. Save & Load functions ───────────────────────────────────────────────────\n",
    "def save_rubric(rubric: list, filepath: str):\n",
    "    \"\"\"Save rubric (list of dicts) to a JSON file.\"\"\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rubric, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Rubric saved to {filepath}\")\n",
    "\n",
    "def load_rubric(filepath: str) -> list:\n",
    "    \"\"\"Load rubric (list of dicts) from a JSON file.\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# ─── 4. Evaluation function ────────────────────────────────────────────────────\n",
    "def evaluate_answer(question: str, answer: str, rubric: list) -> dict:\n",
    "    rubric_json = json.dumps(rubric, indent=2)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\", \"question\", \"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"  (same as criterion)\n",
    "- \"score\" (integer 0–100)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "**Return only the final JSON object** with keys \"scores\" and \"overall_score\".\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(rubric=rubric_json, question=question, answer=answer)\n",
    "    return extract_json(raw)\n",
    "\n",
    "\n",
    "# ─── 5. Example usage ─────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate and save rubric\n",
    "    rubric = generate_rubric(\"Data\\Technical\\dataset.csv\")\n",
    "    save_rubric(rubric, \"Data\\Technical\\rubric.json\")\n",
    "\n",
    "    # Later on, or in another script, simply:\n",
    "    rubric = load_rubric(\"Data\\Technical\\rubric.json\")\n",
    "\n",
    "    # Evaluate a new Q/A\n",
    "    new_q = \"How would you optimize a Python loop over large datasets?\"\n",
    "    new_a = \"You can vectorize with NumPy, leverage Cython for hotspots, or use multiprocessing for parallel batches.\"\n",
    "    report = evaluate_answer(new_q, new_a, rubric)\n",
    "    print(json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe1aba",
   "metadata": {},
   "source": [
    "### run criteria on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results written to evaluation_results.json and evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helper: robust JSON extraction ─────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    \"\"\"\n",
    "    Find the first JSON object or array in `text` by bracket matching.\n",
    "    Returns the parsed JSON. Raises ValueError if none found or unbalanced.\n",
    "    \"\"\"\n",
    "    for start_idx, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_char = ch\n",
    "            close_char = \"}\" if ch == \"{\" else \"]\"\n",
    "            balance = 0\n",
    "            for end_idx in range(start_idx, len(text)):\n",
    "                if text[end_idx] == open_char:\n",
    "                    balance += 1\n",
    "                elif text[end_idx] == close_char:\n",
    "                    balance -= 1\n",
    "                    if balance == 0:\n",
    "                        snippet = text[start_idx : end_idx + 1]\n",
    "                        return json.loads(snippet)\n",
    "            break\n",
    "    raise ValueError(f\"No complete JSON object/array found in LLM output:\\n{text}\")\n",
    "\n",
    "# ─── LLM setup ─────────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"Azure\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "\n",
    "# ─── Evaluation function ───────────────────────────────────────────────────────\n",
    "def evaluate_answer(question: str, answer: str, rubric: list) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single question/answer and a rubric (list of {\"name\",\"description\"}),\n",
    "    prompts the LLM to score each criterion 0–100 and give a one-sentence explanation.\n",
    "    Returns a dict: {\n",
    "      \"scores\": [ { \"name\":\"\", \"score\":int, \"explanation\":\"\" }, … ],\n",
    "      \"overall_score\": float\n",
    "    }\n",
    "    \"\"\"\n",
    "    rubric_json = json.dumps(rubric, indent=2)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\", \"question\", \"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"      (same as criterion)\n",
    "- \"score\"     (integer between 0 and 100)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "Return **only** the final JSON object with keys \"scores\" and \"overall_score\".\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(rubric=rubric_json, question=question, answer=answer)\n",
    "    return extract_json(raw)\n",
    "\n",
    "# ─── 1. Load rubric from JSON file ─────────────────────────────────────────────\n",
    "with open(\"Data\\Technical\\rubric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rubric = json.load(f)\n",
    "\n",
    "# ─── 2. Load dataset of Q/A pairs ─────────────────────────────────────────────\n",
    "df = pd.read_csv(\"Data\\Technical\\dataset.csv\").dropna(subset=[\"question\", \"answer\"])\n",
    "\n",
    "# ─── 3. Iterate and evaluate ─────────────────────────────────────────────────\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    q = row[\"question\"]\n",
    "    a = row[\"answer\"]\n",
    "\n",
    "    try:\n",
    "        report = evaluate_answer(q, a, rubric)\n",
    "    except Exception as e:\n",
    "        # in case of API or parsing errors, record the exception\n",
    "        report = {\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": a,\n",
    "        \"evaluation\": report\n",
    "    })\n",
    "\n",
    "# ─── 4. Save full JSON report ─────────────────────────────────────────────────\n",
    "with open(\"Data\\Technical\\evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ─── 5. Flatten for CSV ────────────────────────────────────────────────────────\n",
    "# We'll make one row per criterion per Q/A, plus the overall score\n",
    "rows = []\n",
    "for entry in results:\n",
    "    if \"evaluation\" in entry and \"scores\" in entry[\"evaluation\"]:\n",
    "        for crit in entry[\"evaluation\"][\"scores\"]:\n",
    "            rows.append({\n",
    "                \"question\": entry[\"question\"],\n",
    "                \"answer\": entry[\"answer\"],\n",
    "                \"criterion\": crit[\"name\"],\n",
    "                \"score\": crit[\"score\"],\n",
    "                \"explanation\": crit[\"explanation\"],\n",
    "                \"overall_score\": entry[\"evaluation\"].get(\"overall_score\")\n",
    "            })\n",
    "    else:\n",
    "        # if an error occurred, record it\n",
    "        rows.append({\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"answer\": entry[\"answer\"],\n",
    "            \"criterion\": None,\n",
    "            \"score\": None,\n",
    "            \"explanation\": None,\n",
    "            \"overall_score\": None,\n",
    "            \"error\": entry[\"evaluation\"].get(\"error\")\n",
    "        })\n",
    "\n",
    "df_flat = pd.DataFrame(rows)\n",
    "df_flat.to_csv(\"Data\\Technical\\evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"Done! Results written to evaluation_results.json and evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829a14d",
   "metadata": {},
   "source": [
    "### Evaluate New upcomming Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e170a89",
   "metadata": {},
   "source": [
    "- first we take the question of the user \n",
    "- then benshof law mawgod fel dataset\n",
    "- law el question mawgod est5dem 70% el question dah be egabto men el dataset w 30% men el criteria\n",
    "    * el kalam dah hay7sal in case eno el question ely fel dataset aslan gayeb score fo2 el 70% based on this criteria \"evaluation_results_dataset.csv\" goaha da\n",
    "- law el question mesh mawgod\n",
    "- shof el top relvant questions hal orybyn meno wala la2\n",
    "- law orybyn est5dmhom homa ma3a el criteria 30% leyhom w 70% criteria\n",
    "- law el top relvant questions mesh relevant\n",
    "- 5alas use el criteria bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72508798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koty9\\AppData\\Local\\Temp\\ipykernel_29312\\1399285732.py:134: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(q_new)\n",
      "C:\\Users\\koty9\\AppData\\Local\\Temp\\ipykernel_29312\\1399285732.py:136: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  match_out = match_chain.run(q_new).strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ No exact match. Checking relevance of top-3 retrieved questions…\n",
      "Relevant retrieved questions: []\n",
      "None relevant → using criteria only:\n",
      "{\n",
      "  \"scores\": [\n",
      "    {\n",
      "      \"name\": \"Clarity\",\n",
      "      \"score\": 80,\n",
      "      \"explanation\": \"The answer is clear and easy to understand.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Accuracy\",\n",
      "      \"score\": 100,\n",
      "      \"explanation\": \"The answer is factually correct, providing the name asked for.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Completeness\",\n",
      "      \"score\": 50,\n",
      "      \"explanation\": \"The answer provides the name but lacks any additional context or detail.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Relevance\",\n",
      "      \"score\": 100,\n",
      "      \"explanation\": \"The answer directly addresses the question asked.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Depth\",\n",
      "      \"score\": 30,\n",
      "      \"explanation\": \"The answer lacks detail beyond a basic response.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Conciseness\",\n",
      "      \"score\": 100,\n",
      "      \"explanation\": \"The answer is succinct and avoids unnecessary verbosity.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Engagement\",\n",
      "      \"score\": 40,\n",
      "      \"explanation\": \"The answer could be more engaging with additional context.\"\n",
      "    }\n",
      "  ],\n",
      "  \"overall_score\": 71\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helpers ───────────────────────────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    for i,ch in enumerate(text):\n",
    "        if ch in (\"{\",\"[\"):\n",
    "            open_c, close_c = (ch, \"}\" if ch==\"{\" else \"]\")\n",
    "            bal = 0\n",
    "            for j in range(i, len(text)):\n",
    "                if text[j]==open_c:   bal+=1\n",
    "                elif text[j]==close_c: bal-=1\n",
    "                if bal==0:\n",
    "                    return json.loads(text[i:j+1])\n",
    "    raise ValueError(\"No valid JSON in LLM output.\")\n",
    "\n",
    "def evaluate_answer(question: str, answer: str, rubric: list) -> dict:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\",\"question\",\"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"       \n",
    "- \"score\"      (0–100 integer)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "Return **only** the JSON:\n",
    "{{ \"scores\":[…], \"overall_score\":… }}\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(\n",
    "        rubric=json.dumps(rubric,indent=2),\n",
    "        question=question,\n",
    "        answer=answer\n",
    "    )\n",
    "    return extract_json(raw)\n",
    "\n",
    "\n",
    "# ─── Setup ─────────────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_TYPE\"]      = \"Azure\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"]= os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# LLM & Embeddings\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Load dataset & build FAISS index on questions\n",
    "df = pd.read_csv(\"Data\\Technical\\dataset.csv\")\n",
    "questions = df[\"question\"].astype(str).tolist()\n",
    "vectorstore = FAISS.from_texts(questions, embeddings)\n",
    "retriever  = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "# Chain to decide if a new question exactly exists\n",
    "match_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that determines if a new question exactly matches one in the dataset.\n",
    "From these retrieved questions:\n",
    "{context}\n",
    "\n",
    "New Question:\n",
    "{question}\n",
    "\n",
    "Respond with **exactly**:\n",
    "- YES: \"<matched question>\"  \n",
    "- NO\n",
    "\"\"\",\n",
    ")\n",
    "match_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": match_prompt}\n",
    ")\n",
    "\n",
    "# Chain to ask which of the top-3 are truly relevant\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_question\",\"candidates\"],\n",
    "    template=\"\"\"\n",
    "New question:\n",
    "{new_question}\n",
    "\n",
    "Here are 3 candidate questions retrieved from the dataset:\n",
    "{candidates}\n",
    "\n",
    "For each candidate, respond with YES or NO if it's truly relevant to the new question.\n",
    "Return a JSON array of only those candidate strings that are relevant.\n",
    "Example output: [\"Q1 text\", \"Q3 text\"]\n",
    "\"\"\",\n",
    ")\n",
    "relevance_chain = LLMChain(llm=llm, prompt=relevance_prompt)\n",
    "\n",
    "# Load rubric (for evaluation)\n",
    "with open(r\"Data\\Technical\\rubric.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    rubric = json.load(f)\n",
    "\n",
    "\n",
    "# ─── Main logic ────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    q_new = input(\"Enter your new question:\\n> \").strip()\n",
    "    a_new = input(\"\\nEnter the answer to evaluate:\\n> \").strip()\n",
    "\n",
    "    # 1) Exact-match check\n",
    "    docs = retriever.get_relevant_documents(q_new)\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "    match_out = match_chain.run(q_new).strip()\n",
    "\n",
    "    if match_out.upper().startswith(\"YES\"):\n",
    "        # Found exact Q in dataset\n",
    "        m = re.search(r'YES:\\s*\"(.*)\"', match_out)\n",
    "        q_match = m.group(1) if m else None\n",
    "        print(f\"\\n→ Exact match in dataset: “{q_match}”\")\n",
    "\n",
    "        # get its past overall score if available\n",
    "        # (assumes you've pre-computed evaluation_results.json)\n",
    "        with open(\"Data\\Technical\\evaluation_results.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "            past = json.load(f)\n",
    "        past_scores = {e[\"question\"]: e[\"evaluation\"].get(\"overall_score\",0) for e in past}\n",
    "        ds_score = past_scores.get(q_match, 0.0)\n",
    "        print(f\"Dataset Q/A overall score: {ds_score:.2f}/100\")\n",
    "\n",
    "        if ds_score > 70:\n",
    "            print(\"Combining 70% dataset score + 30% fresh rubric evaluation…\")\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            rub_score = new_eval[\"overall_score\"]\n",
    "            combined = 0.7*ds_score + 0.3*rub_score\n",
    "            print(f\"Rubric eval: {rub_score:.2f}/100  → Combined: {combined:.2f}/100\")\n",
    "            print(\"\\nFull rubric breakdown:\")\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "        else:\n",
    "            print(\"Dataset score ≤70%, using criteria only:\")\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "\n",
    "    else:\n",
    "        # 2) No exact match → check relevance of top-3\n",
    "        print(\"\\n→ No exact match. Checking relevance of top-3 retrieved questions…\")\n",
    "        candidates = json.dumps([d.page_content for d in docs], indent=2)\n",
    "        rel_raw = relevance_chain.run(new_question=q_new, candidates=candidates)\n",
    "        relevant = extract_json(rel_raw)\n",
    "        print(f\"Relevant retrieved questions: {relevant}\")\n",
    "\n",
    "        if not relevant:\n",
    "            print(\"None relevant → using criteria only:\")\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "        else:\n",
    "            # 3) Evaluate each relevant Q/A from dataset\n",
    "            scores = []\n",
    "            for q_old in relevant:\n",
    "                ans_old = df.loc[df.question==q_old, \"answer\"].iloc[0]\n",
    "                eval_old = evaluate_answer(q_old, ans_old, rubric)\n",
    "                scores.append(eval_old[\"overall_score\"])\n",
    "            avg_old = sum(scores)/len(scores)\n",
    "\n",
    "            # 4) Fresh rubric score for new answer\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            rub_score = new_eval[\"overall_score\"]\n",
    "\n",
    "            # 5) Combine 70% rubric + 30% avg relevant\n",
    "            combined = 0.7*rub_score + 0.3*avg_old\n",
    "            print(f\"\\nRubric eval (70%): {rub_score:.2f}/100\")\n",
    "            print(f\"Avg relevant Q/A eval (30%): {avg_old:.2f}/100\")\n",
    "            print(f\"Combined final score: {combined:.2f}/100\")\n",
    "            print(\"\\nFull rubric breakdown of your answer:\")\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462a6bc",
   "metadata": {},
   "source": [
    "## HR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e9d51",
   "metadata": {},
   "source": [
    "### Create a concrete‐answer HR dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7e8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete‐answer dataset saved to Data\\HR\\interview_best_answers_samples.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helper: robust JSON extraction ─────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    for start_idx, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_char, close_char = (ch, \"}\" if ch==\"{\" else \"]\")\n",
    "            balance = 0\n",
    "            for end_idx in range(start_idx, len(text)):\n",
    "                if text[end_idx] == open_char:\n",
    "                    balance += 1\n",
    "                elif text[end_idx] == close_char:\n",
    "                    balance -= 1\n",
    "                    if balance == 0:\n",
    "                        return json.loads(text[start_idx:end_idx+1])\n",
    "    raise ValueError(f\"No complete JSON object/array found in:\\n{text}\")\n",
    "\n",
    "# ─── Helper: detect “instructional” style answers ──────────────────────────────\n",
    "def is_instructional(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Very simple heuristic: if the answer begins with common\n",
    "    imperative phrases or contains KEYWORDS like “Best strategy”,\n",
    "    “Remember that…”, “Example:…”, etc., assume it’s meta‐instructions\n",
    "    rather than a direct answer. Adjust these patterns as needed.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"^\\s*(Start with|Remember that|BEST ANSWERS?|Best strategy|Example:|Remember, you|To answer this question|If you want to|The only right answer|To cover both|Many executives)\",\n",
    "        r\"\\b(you should|you must|always|never|exercise)\\b\"\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        if re.search(pat, text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ─── Helper: convert “instructions” → a concrete sample answer ────────────────\n",
    "def convert_to_sample_answer(question: str, instructions: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    When the raw “answer” is really a block of instructions,\n",
    "    call the LLM to produce one short, direct example answer to that question.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"instructions\"],\n",
    "        template=\"\"\"\n",
    "Here is an HR interview question:\n",
    "{question}\n",
    "\n",
    "Here are some instructions about *how* to answer that question in a perfectly effective way:\n",
    "{instructions}\n",
    "\n",
    "Please write a brief (1–2 paragraph) example answer to the question, \n",
    "directly implementing those instructions.\n",
    "**Do not repeat the instructions.** Produce only the final, concrete answer.\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    sample = chain.run(question=question, instructions=instructions).strip()\n",
    "    return sample\n",
    "\n",
    "# ─── 1. Load credentials ───────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]  = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_TYPE\"]       = \"Azure\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "\n",
    "# ─── 2. Load original HR Q/A CSV and preprocess to concrete answers ────────────\n",
    "ORIGINAL_CSV = r\"Data\\HR\\interview_best_answers_cleaned.csv\"\n",
    "SAMPLE_CSV   = r\"Data\\HR\\interview_best_answers_samples.csv\"\n",
    "\n",
    "df_orig = pd.read_csv(ORIGINAL_CSV).dropna(subset=[\"question\", \"answer\"])\n",
    "df_samples = df_orig.copy()\n",
    "\n",
    "for idx, row in df_samples.iterrows():\n",
    "    ans = row[\"answer\"]\n",
    "    if is_instructional(ans):\n",
    "        try:\n",
    "            concrete = convert_to_sample_answer(row[\"question\"], ans, llm)\n",
    "        except Exception:\n",
    "            concrete = ans  # fallback if LLM call fails\n",
    "        df_samples.at[idx, \"answer\"] = concrete\n",
    "\n",
    "# ─── 3. Save the new “samples” CSV ──────────────────────────────────────────────\n",
    "os.makedirs(os.path.dirname(SAMPLE_CSV), exist_ok=True)\n",
    "df_samples.to_csv(SAMPLE_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Concrete‐answer dataset saved to {SAMPLE_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c13a1",
   "metadata": {},
   "source": [
    "### Make the criteria (for HR Q/A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26af7747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rubric saved to Data\\HR\\hr_rubric.json\n",
      "{\n",
      "  \"scores\": [\n",
      "    {\n",
      "      \"name\": \"Relevance\",\n",
      "      \"score\": 90,\n",
      "      \"explanation\": \"The answer directly addresses the question by outlining a clear approach to handling conflict.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Clarity\",\n",
      "      \"score\": 85,\n",
      "      \"explanation\": \"The response is clear and concise, detailing a straightforward process.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Professionalism\",\n",
      "      \"score\": 80,\n",
      "      \"explanation\": \"The tone is professional and appropriate for an HR interview.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Depth of Insight\",\n",
      "      \"score\": 75,\n",
      "      \"explanation\": \"While the response offers a basic strategy, it lacks detailed examples or insights.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Positivity\",\n",
      "      \"score\": 85,\n",
      "      \"explanation\": \"The answer is positively framed, focusing on collaboration and shared goals.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Engagement\",\n",
      "      \"score\": 70,\n",
      "      \"explanation\": \"The response does not explicitly invite further discussion or engagement.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Adaptability\",\n",
      "      \"score\": 80,\n",
      "      \"explanation\": \"The answer shows adaptability by mentioning finding compromises aligned with goals.\"\n",
      "    }\n",
      "  ],\n",
      "  \"overall_score\": 80.71\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helper: robust JSON extraction ─────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    for start_idx, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_char, close_char = (ch, \"}\" if ch==\"{\" else \"]\")\n",
    "            balance = 0\n",
    "            for end_idx in range(start_idx, len(text)):\n",
    "                if text[end_idx] == open_char:\n",
    "                    balance += 1\n",
    "                elif text[end_idx] == close_char:\n",
    "                    balance -= 1\n",
    "                    if balance == 0:\n",
    "                        return json.loads(text[start_idx:end_idx+1])\n",
    "    raise ValueError(f\"No complete JSON object/array found in:\\n{text}\")\n",
    "\n",
    "# ─── 1. Load credentials ───────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]  = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_TYPE\"]       = \"Azure\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "\n",
    "# ─── 2. Rubric generation ─────────────────────────────────────────────────────\n",
    "def generate_rubric(csv_path: str, max_examples: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Reads the concrete‐answer HR Q/A CSV (with columns \"question\" and \"answer\"),\n",
    "    then prompts the LLM to produce a JSON array of evaluation criteria.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path).dropna(subset=[\"question\", \"answer\"])\n",
    "    samples = df.head(max_examples)\n",
    "    context = \"\\n\\n\".join(f\"Q: {q}\\nA: {a}\" for q, a in zip(samples[\"question\"], samples[\"answer\"]))\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"\n",
    "You are an expert evaluator. Here are sample HR Q/A pairs:\n",
    "{context}\n",
    "\n",
    "Generate a JSON **array** of 5–8 evaluation criteria tailored for HR question/answer quality. \n",
    "Each criterion must be an object with:\n",
    "- \"name\": short title\n",
    "- \"description\": one-sentence explanation\n",
    "\n",
    "**Output only the JSON array**, without any extra commentary.\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(context=context)\n",
    "    return extract_json(raw)\n",
    "\n",
    "# ─── 3. Save & Load functions ───────────────────────────────────────────────────\n",
    "def save_rubric(rubric: list, filepath: str):\n",
    "    \"\"\"Save rubric (list of dicts) to a JSON file.\"\"\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rubric, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Rubric saved to {filepath}\")\n",
    "\n",
    "def load_rubric(filepath: str) -> list:\n",
    "    \"\"\"Load rubric (list of dicts) from a JSON file.\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ─── 4. Evaluation function ────────────────────────────────────────────────────\n",
    "def evaluate_answer(question: str, answer: str, rubric: list) -> dict:\n",
    "    \"\"\"\n",
    "    Using the provided rubric, prompt the LLM to score the given HR answer.\n",
    "    Returns a dict with \"scores\" (list of {name, score, explanation})\n",
    "    and \"overall_score\" (float).\n",
    "    \"\"\"\n",
    "    rubric_json = json.dumps(rubric, indent=2)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\", \"question\", \"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this HR response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"       (same as criterion name)\n",
    "- \"score\"      (integer 0–100)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "Return **only** the final JSON object with keys \"scores\" and \"overall_score\".\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(rubric=rubric_json, question=question, answer=answer)\n",
    "    return extract_json(raw)\n",
    "\n",
    "# ─── 5. Example usage ─────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    SAMPLE_CSV = r\"Data\\HR\\interview_best_answers_samples.csv\"\n",
    "    # Generate and save rubric from the concrete‐answer HR dataset\n",
    "    rubric = generate_rubric(SAMPLE_CSV)\n",
    "    save_rubric(rubric, r\"Data\\HR\\hr_rubric.json\")\n",
    "\n",
    "    # Later on, or in another script, simply:\n",
    "    rubric = load_rubric(r\"Data\\HR\\hr_rubric.json\")\n",
    "\n",
    "    # Evaluate a new HR Q/A\n",
    "    new_q = \"How do you handle conflict in a team setting?\"\n",
    "    new_a = \"I first listen actively to both sides, encourage open communication, and then work with them to find a compromise that aligns with our shared goals.\"\n",
    "    report = evaluate_answer(new_q, new_a, rubric)\n",
    "    print(json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955d66a",
   "metadata": {},
   "source": [
    "### Run criteria on the HR dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f86fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! HR results written to hr_evaluation_results_with_samples.json and hr_evaluation_results_with_samples.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helper: robust JSON extraction ─────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    \"\"\"\n",
    "    Find the first JSON object or array in `text` by bracket matching.\n",
    "    Returns the parsed JSON. Raises ValueError if none found or unbalanced.\n",
    "    \"\"\"\n",
    "    for start_idx, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_char = ch\n",
    "            close_char = \"}\" if ch == \"{\" else \"]\"\n",
    "            balance = 0\n",
    "            for end_idx in range(start_idx, len(text)):\n",
    "                if text[end_idx] == open_char:\n",
    "                    balance += 1\n",
    "                elif text[end_idx] == close_char:\n",
    "                    balance -= 1\n",
    "                    if balance == 0:\n",
    "                        snippet = text[start_idx : end_idx + 1]\n",
    "                        return json.loads(snippet)\n",
    "            break\n",
    "    raise ValueError(f\"No complete JSON object/array found in LLM output:\\n{text}\")\n",
    "\n",
    "# ─── New helper: detect “instructional” style answers ──────────────────────────\n",
    "def is_instructional(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Very simple heuristic: if the answer begins with common\n",
    "    imperative phrases or contains KEYWORDS like “Best strategy”,\n",
    "    “Remember that…”, “Example:…”, etc., assume it’s meta‐instructions\n",
    "    rather than a direct answer. Adjust these patterns as needed.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"^\\s*(Start with|Remember that|BEST ANSWERS?|Best strategy|Example:|Remember, you|To answer this question|If you want to|The only right answer|To cover both|Many executives)\",\n",
    "        r\"\\b(you should|you must|always|never|exercise)\\b\"\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        if re.search(pat, text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ─── New helper: convert “instructions” → a concrete sample answer ─────────────\n",
    "def convert_to_sample_answer(question: str, instructions: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    When the raw “answer” is really a block of instructions,\n",
    "    call the LLM to produce one short, direct example answer to that question.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"instructions\"],\n",
    "        template=\"\"\"\n",
    "Here is an HR interview question:\n",
    "{question}\n",
    "\n",
    "Here are some instructions about *how* to answer that question in a perfectly effective way:\n",
    "{instructions}\n",
    "\n",
    "Please write a brief (1–2 paragraph) example answer to the question, \n",
    "directly implementing those instructions.\n",
    "**Do not repeat the instructions.** Produce only the final, concrete answer.\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    sample = chain.run(question=question, instructions=instructions).strip()\n",
    "    return sample\n",
    "\n",
    "# ─── LLM setup ─────────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]  = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_TYPE\"]       = \"Azure\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "\n",
    "# ─── Evaluation function ───────────────────────────────────────────────────────\n",
    "def evaluate_answer(question: str, answer: str, rubric: list) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single HR question/answer and a rubric (list of {\"name\",\"description\"}),\n",
    "    prompts the LLM to score each criterion 0–100 and give a one-sentence explanation.\n",
    "    Returns a dict: {\n",
    "      \"scores\": [ { \"name\":\"\", \"score\":int, \"explanation\":\"\" }, … ],\n",
    "      \"overall_score\": float\n",
    "    }\n",
    "    \"\"\"\n",
    "    rubric_json = json.dumps(rubric, indent=2)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\", \"question\", \"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this HR response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"       (same as criterion)\n",
    "- \"score\"      (integer between 0 and 100)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "Return **only** the final JSON object with keys \"scores\" and \"overall_score\".\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(rubric=rubric_json, question=question, answer=answer)\n",
    "    return extract_json(raw)\n",
    "\n",
    "# ─── 1. Load rubric from JSON file ─────────────────────────────────────────────\n",
    "with open(r\"Data\\HR\\hr_rubric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rubric = json.load(f)\n",
    "\n",
    "# ─── 2. Load concrete‐answer HR dataset of Q/A pairs ───────────────────────────\n",
    "df = pd.read_csv(r\"Data\\HR\\interview_best_answers_samples.csv\").dropna(subset=[\"question\", \"answer\"])\n",
    "\n",
    "# ─── 3. Iterate, convert if needed, and evaluate ──────────────────────────────\n",
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    q = row[\"question\"]\n",
    "    raw_ans = row[\"answer\"]\n",
    "\n",
    "    # If the “answer” is really a set of instructions, convert to a sample answer\n",
    "    if is_instructional(raw_ans):\n",
    "        try:\n",
    "            concrete_ans = convert_to_sample_answer(q, raw_ans, llm)\n",
    "        except Exception:\n",
    "            concrete_ans = raw_ans  # fallback if generation fails\n",
    "    else:\n",
    "        concrete_ans = raw_ans\n",
    "\n",
    "    try:\n",
    "        report = evaluate_answer(q, concrete_ans, rubric)\n",
    "    except Exception as e:\n",
    "        report = {\"error\": str(e)}\n",
    "\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"raw_answer\": raw_ans,\n",
    "        \"concrete_answer_used\": concrete_ans,\n",
    "        \"evaluation\": report\n",
    "    })\n",
    "\n",
    "# ─── 4. Save full JSON report ─────────────────────────────────────────────────\n",
    "with open(r\"Data\\HR\\hr_evaluation_results_with_samples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ─── 5. Flatten for CSV ────────────────────────────────────────────────────────\n",
    "rows = []\n",
    "for entry in results:\n",
    "    q = entry[\"question\"]\n",
    "    a_raw = entry[\"raw_answer\"]\n",
    "    a_used = entry[\"concrete_answer_used\"]\n",
    "    eval_block = entry[\"evaluation\"]\n",
    "\n",
    "    if \"scores\" in eval_block:\n",
    "        for crit in eval_block[\"scores\"]:\n",
    "            rows.append({\n",
    "                \"question\": q,\n",
    "                \"raw_answer\": a_raw,\n",
    "                \"concrete_answer\": a_used,\n",
    "                \"criterion\": crit[\"name\"],\n",
    "                \"score\": crit[\"score\"],\n",
    "                \"explanation\": crit[\"explanation\"],\n",
    "                \"overall_score\": eval_block.get(\"overall_score\")\n",
    "            })\n",
    "    else:\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"raw_answer\": a_raw,\n",
    "            \"concrete_answer\": a_used,\n",
    "            \"criterion\": None,\n",
    "            \"score\": None,\n",
    "            \"explanation\": None,\n",
    "            \"overall_score\": None,\n",
    "            \"error\": eval_block.get(\"error\")\n",
    "        })\n",
    "\n",
    "df_flat = pd.DataFrame(rows)\n",
    "df_flat.to_csv(r\"Data\\HR\\hr_evaluation_results_with_samples.csv\", index=False)\n",
    "\n",
    "print(\"Done! HR results written to hr_evaluation_results_with_samples.json and hr_evaluation_results_with_samples.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b674b14",
   "metadata": {},
   "source": [
    "### Evaluate a New HR Question Using Past Dataset + Criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4300f57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ No exact match. Checking relevance of top‐3 retrieved questions…\n",
      "Relevant retrieved questions: ['What do you worry about?']\n",
      "\n",
      " • Rubric eval (70%):          24.00/100\n",
      " • Avg relevant old eval (30%): 85.00/100\n",
      " → Combined final score:       42.30/100\n",
      "\n",
      "Full rubric breakdown for the new answer:\n",
      "{\n",
      "  \"scores\": [\n",
      "    {\n",
      "      \"name\": \"Relevance\",\n",
      "      \"score\": 50,\n",
      "      \"explanation\": \"The answer addresses the question but lacks detail.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Clarity\",\n",
      "      \"score\": 30,\n",
      "      \"explanation\": \"The response is unclear due to language and phrasing.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Professionalism\",\n",
      "      \"score\": 20,\n",
      "      \"explanation\": \"The tone is informal and not suitable for an HR interview.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Depth of Insight\",\n",
      "      \"score\": 10,\n",
      "      \"explanation\": \"The answer provides minimal insight into the candidate's qualifications.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Positivity\",\n",
      "      \"score\": 40,\n",
      "      \"explanation\": \"The answer does not explicitly frame the fear positively.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Engagement\",\n",
      "      \"score\": 10,\n",
      "      \"explanation\": \"The response does not invite further discussion or engagement.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Adaptability\",\n",
      "      \"score\": 10,\n",
      "      \"explanation\": \"The answer does not demonstrate adaptability to the role.\"\n",
      "    }\n",
      "  ],\n",
      "  \"overall_score\": 24\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# ─── Helpers ───────────────────────────────────────────────────────────────────\n",
    "def extract_json(text: str):\n",
    "    \"\"\"\n",
    "    Locate the first complete JSON object or array in `text` by bracket matching.\n",
    "    Returns the parsed JSON. Raises ValueError if none found.\n",
    "    \"\"\"\n",
    "    for i, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_c, close_c = (ch, \"}\" if ch == \"{\" else \"]\")\n",
    "            bal = 0\n",
    "            for j in range(i, len(text)):\n",
    "                if text[j] == open_c:\n",
    "                    bal += 1\n",
    "                elif text[j] == close_c:\n",
    "                    bal -= 1\n",
    "                    if bal == 0:\n",
    "                        return json.loads(text[i : j + 1])\n",
    "    raise ValueError(\"No valid JSON in LLM output.\")\n",
    "\n",
    "def is_instructional(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if the provided HR answer text is actually a set of instructions\n",
    "    rather than a concrete sample answer. For example, many entries begin\n",
    "    with \"BEST ANSWERS:\" or contain bullet‐style guidelines.\n",
    "    \"\"\"\n",
    "    t = text.strip().lower()\n",
    "    return t.startswith(\"best answers:\") or \"instructions\" in t or \"strategy\" in t\n",
    "\n",
    "def convert_to_sample_answer(question: str, instruction: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    Given an HR question and a block of instructional text, ask the LLM\n",
    "    to produce a concrete sample answer that follows those instructions.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"instruction\", \"question\"],\n",
    "        template=\"\"\"\n",
    "You are a helpful assistant. Based on these instructions, generate a concrete, polished\n",
    "sample answer to the HR interview question.\n",
    "\n",
    "Instructions:\n",
    "{instruction}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Provide the resulting sample answer (1–3 paragraphs) that follows the instructions.\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return chain.run(instruction=instruction, question=question).strip()\n",
    "\n",
    "# ─── Setup ─────────────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_TYPE\"]      = \"Azure\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"]= os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# ─── Load HR dataset & build FAISS index on questions ─────────────────────────\n",
    "HR_CSV_PATH = r\"Data\\HR\\interview_best_answers_samples.csv\"\n",
    "df = pd.read_csv(HR_CSV_PATH)\n",
    "questions = df[\"question\"].astype(str).tolist()\n",
    "vectorstore = FAISS.from_texts(questions, embeddings)\n",
    "retriever  = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ─── Chain: EXACT‐MATCH check ──────────────────────────────────────────────────\n",
    "match_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that determines if a new HR question exactly matches one in the dataset.\n",
    "From these retrieved questions:\n",
    "{context}\n",
    "\n",
    "New Question:\n",
    "{question}\n",
    "\n",
    "Respond with exactly:\n",
    "- YES: \"<matched question>\"\n",
    "- NO\n",
    "\"\"\",\n",
    ")\n",
    "match_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": match_prompt}\n",
    ")\n",
    "\n",
    "# ─── Chain: TOP‐3 RELEVANCE check ───────────────────────────────────────────────\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_question\", \"candidates\"],\n",
    "    template=\"\"\"\n",
    "New question:\n",
    "{new_question}\n",
    "\n",
    "Here are 3 candidate questions retrieved from the HR dataset:\n",
    "{candidates}\n",
    "\n",
    "For each candidate, respond with YES or NO if it's truly relevant to the new question.\n",
    "Return a JSON array of only those candidate strings that are relevant.\n",
    "Example output: [\"Tell me about yourself.\", \"What are your strengths?\"]\n",
    "\"\"\",\n",
    ")\n",
    "relevance_chain = LLMChain(llm=llm, prompt=relevance_prompt)\n",
    "\n",
    "# ─── Load HR rubric & past evaluation results ─────────────────────────────────\n",
    "RUBRIC_PATH = r\"Data\\HR\\hr_rubric.json\"\n",
    "EVAL_JSON   = r\"Data\\HR\\hr_evaluation_results_with_samples.json\"\n",
    "with open(RUBRIC_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    rubric = json.load(f)\n",
    "\n",
    "with open(EVAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    past = json.load(f)\n",
    "\n",
    "# Precompute a map: question → old overall_score\n",
    "past_scores = {\n",
    "    entry[\"question\"]: entry[\"evaluation\"].get(\"overall_score\", 0.0)\n",
    "    for entry in past\n",
    "}\n",
    "\n",
    "# ─── Helpers (reused) ──────────────────────────────────────────────────────────\n",
    "def evaluate_answer(question: str, answer: str, rubric: list) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single HR question/answer and a rubric (list of {\"name\",\"description\"}),\n",
    "    prompts the LLM to score each criterion 0–100 and give a one-sentence explanation.\n",
    "    Returns a dict: { \"scores\": […], \"overall_score\": … }\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\", \"question\", \"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this HR response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"       (same as criterion)\n",
    "- \"score\"      (0–100 integer)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "Return **only** the JSON:\n",
    "{{ \"scores\":[…], \"overall_score\":… }}\n",
    "\"\"\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    raw = chain.run(\n",
    "        rubric=json.dumps(rubric, indent=2),\n",
    "        question=question,\n",
    "        answer=answer\n",
    "    )\n",
    "    return extract_json(raw)\n",
    "\n",
    "# ─── Main interactive loop ────────────────────────────────────────────────────\n",
    "def main():\n",
    "    q_new = input(\"Enter your new HR question:\\n> \").strip()\n",
    "    a_new = input(\"\\nEnter the answer to evaluate:\\n> \").strip()\n",
    "\n",
    "    # 1) EXACT‐MATCH check\n",
    "    docs = retriever.get_relevant_documents(q_new)\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "    match_out = match_chain.run(q_new).strip()\n",
    "\n",
    "    if match_out.upper().startswith(\"YES\"):\n",
    "        # Found an exact match in the HR dataset\n",
    "        m = re.search(r'YES:\\s*\"(.*)\"', match_out)\n",
    "        q_match = m.group(1) if m else None\n",
    "        print(f\"\\n→ Exact match in HR dataset: “{q_match}”\")\n",
    "\n",
    "        # Retrieve its old overall score\n",
    "        ds_score = past_scores.get(q_match, 0.0)\n",
    "        print(f\"Dataset Q/A overall score: {ds_score:.2f}/100\")\n",
    "\n",
    "        if ds_score > 70:\n",
    "            print(\"Combining 70% dataset score + 30% fresh rubric evaluation…\")\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            rub_score = new_eval[\"overall_score\"]\n",
    "            combined = 0.7 * ds_score + 0.3 * rub_score\n",
    "            print(f\" • Rubric eval: {rub_score:.2f}/100\")\n",
    "            print(f\" • Combined:    {combined:.2f}/100\\n\")\n",
    "            print(\"Full rubric breakdown:\")\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "        else:\n",
    "            print(\"Dataset score ≤ 70 → using only fresh rubric evaluation:\")\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "\n",
    "    else:\n",
    "        # 2) NO EXACT MATCH → do TOP‐3 RELEVANCE\n",
    "        print(\"\\n→ No exact match. Checking relevance of top‐3 retrieved questions…\")\n",
    "        candidates = json.dumps([d.page_content for d in docs], indent=2)\n",
    "\n",
    "        rel_raw = relevance_chain.run(new_question=q_new, candidates=candidates)\n",
    "\n",
    "        try:\n",
    "            relevant = extract_json(rel_raw)\n",
    "        except ValueError:\n",
    "            # LLM did not return valid JSON → treat as “no relevant questions”\n",
    "            print(\"⚠️ Warning: could not parse relevance output as JSON.\")\n",
    "            relevant = []\n",
    "\n",
    "        print(f\"Relevant retrieved questions: {relevant}\")\n",
    "\n",
    "        if not relevant:\n",
    "            # 3.a) No relevant Q found → score purely by rubric\n",
    "            print(\"None relevant → using rubric only:\")\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "\n",
    "        else:\n",
    "            # 3.b) Found relevant Qs → average their old scores\n",
    "            scores = []\n",
    "            for q_old in relevant:\n",
    "                # Fetch the old answer from the sample dataset\n",
    "                ans_old = df.loc[df.question == q_old, \"answer\"].iloc[0]\n",
    "\n",
    "                # If that old answer was instructional, generate a concrete sample\n",
    "                if is_instructional(ans_old):\n",
    "                    try:\n",
    "                        ans_old_concrete = convert_to_sample_answer(q_old, ans_old, llm)\n",
    "                    except Exception:\n",
    "                        ans_old_concrete = ans_old\n",
    "                else:\n",
    "                    ans_old_concrete = ans_old\n",
    "\n",
    "                eval_old = evaluate_answer(q_old, ans_old_concrete, rubric)\n",
    "                scores.append(eval_old[\"overall_score\"])\n",
    "            avg_old = sum(scores) / len(scores)\n",
    "\n",
    "            # 4) Now score the new answer by rubric\n",
    "            new_eval = evaluate_answer(q_new, a_new, rubric)\n",
    "            rub_score = new_eval[\"overall_score\"]\n",
    "\n",
    "            # 5) Combine 70% rubric + 30% avg_old\n",
    "            combined = 0.7 * rub_score + 0.3 * avg_old\n",
    "            print(f\"\\n • Rubric eval (70%):          {rub_score:.2f}/100\")\n",
    "            print(f\" • Avg relevant old eval (30%): {avg_old:.2f}/100\")\n",
    "            print(f\" → Combined final score:       {combined:.2f}/100\\n\")\n",
    "            print(\"Full rubric breakdown for the new answer:\")\n",
    "            print(json.dumps(new_eval, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
