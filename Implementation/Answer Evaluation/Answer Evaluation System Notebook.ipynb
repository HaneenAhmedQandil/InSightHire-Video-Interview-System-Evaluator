{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4ae509",
   "metadata": {},
   "source": [
    "# Candidate Evaluation Pipeline\n",
    "\n",
    "This notebook demonstrates a unified pipeline that:\n",
    "1. Loads both Technical and HR evaluation rubrics (pre‐generated).\n",
    "2. Builds FAISS indexes for both Technical and HR question sets.\n",
    "3. Defines helper functions for JSON extraction, rubric‐based scoring, and instructional‐style handling for HR.\n",
    "4. Implements a single `evaluate_question_answer(...)` function that:\n",
    "   - Detects whether a question is Technical or HR.\n",
    "   - Checks for an **exact match** in the corresponding dataset. If found and its old score > 70, combines 70% old + 30% fresh rubric. If old score ≤ 70, uses 100% fresh rubric.\n",
    "   - If no exact match, checks **relevance** among top‐3 FAISS neighbors. If relevant old questions exist, averages their old scores and combines 30% average + 70% fresh rubric. Otherwise, uses 100% fresh rubric.\n",
    "5. Iterates over a master list of eight questions (4 Technical, 4 HR), prompts for the candidate’s answer to each, and prints & saves a JSON summary.\n",
    "\n",
    "Make sure to install required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install pandas python-dotenv faiss-cpu langchain openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cfc7b",
   "metadata": {},
   "source": [
    "## 1. Imports and Helper: Robust JSON Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a255b",
   "metadata": {},
   "source": [
    "### 1.1 Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95995253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1.2 Helper: Robust JSON extraction for LLM outputs\n",
    "def extract_json(text: str):\n",
    "    \"\"\"\n",
    "    Finds the first balanced JSON object or array in `text` (by bracket matching),\n",
    "    then returns the parsed Python object. Raises ValueError if none found.\n",
    "    \"\"\"\n",
    "    for start_idx, ch in enumerate(text):\n",
    "        if ch in (\"{\", \"[\"):\n",
    "            open_char = ch\n",
    "            close_char = \"}\" if ch == \"{\" else \"]\"\n",
    "            balance = 0\n",
    "            for end_idx in range(start_idx, len(text)):\n",
    "                if text[end_idx] == open_char:\n",
    "                    balance += 1\n",
    "                elif text[end_idx] == close_char:\n",
    "                    balance -= 1\n",
    "                    if balance == 0:\n",
    "                        snippet = text[start_idx : end_idx + 1]\n",
    "                        return json.loads(snippet)\n",
    "            break\n",
    "    raise ValueError(f\"No complete JSON object/array found in LLM output:\\n{text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c081be",
   "metadata": {},
   "source": [
    "## 2. Load Environment Variables & Initialize AzureOpenAI Client\n",
    "\n",
    "We load environment variables (`.env`) for AzureOpenAI and instantiate the `AzureChatOpenAI` client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee832e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koty9\\AppData\\Local\\Temp\\ipykernel_31188\\1995403407.py:7: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# 2. LOAD ENV AND AZURE OPENAI CLIENT\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]  = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_TYPE\"]       = \"Azure\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"GPT-4O-50-1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edea74",
   "metadata": {},
   "source": [
    "## 3. Evaluation Helpers\n",
    "\n",
    "- **`evaluate_with_rubric(...)`**: Prompts the LLM with a given rubric (JSON array of criteria) to score a single (question, answer) pair.  \n",
    "- **`is_instructional(...)`**: Heuristic to detect if an HR “answer” is actually a set of meta‐instructions rather than a concrete response.  \n",
    "- **`convert_to_sample_answer(...)`**: If the HR answer is instructional, uses the LLM to produce a concrete 1–2 paragraph sample answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f07c6",
   "metadata": {},
   "source": [
    "### 3.1 Evaluate with rubric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1bf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Evaluate with rubric (3× scoring + summary of rationales)\n",
    "def evaluate_with_rubric(question: str, answer: str, rubric: list) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single question & answer and a rubric (list of {\"name\",\"description\"}),\n",
    "    calls the LLM three times for independent evaluations, then:\n",
    "      - Averages each criterion's numeric \"score\" (rounded to two decimals).\n",
    "      - Summarizes the three one‐sentence rationales into a single concise rationale.\n",
    "      - Computes \"overall_score\" as the average of all averaged criterion scores.\n",
    "    Returns a JSON object:\n",
    "      {\n",
    "        \"scores\": [\n",
    "          {\n",
    "            \"name\": <criterion name>,\n",
    "            \"score\": <avg of 3 runs>,\n",
    "            \"explanation\": <summarized rationale>\n",
    "          },\n",
    "          ...\n",
    "        ],\n",
    "        \"overall_score\": <float>\n",
    "      }\n",
    "    \"\"\"\n",
    "    # 1) Prepare the base prompt template (identical for all runs)\n",
    "    rubric_str = json.dumps(rubric, indent=2)\n",
    "    eval_prompt = PromptTemplate(\n",
    "        input_variables=[\"rubric\", \"question\", \"answer\"],\n",
    "        template=\"\"\"\n",
    "You are an objective assessor. Here is a rubric (JSON array):\n",
    "{rubric}\n",
    "\n",
    "Now evaluate this response.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "For each rubric item, produce an object with:\n",
    "- \"name\"       (same as criterion)\n",
    "- \"score\"      (integer 0–100)\n",
    "- \"explanation\" (one-sentence rationale)\n",
    "\n",
    "Then compute \"overall_score\" as the average of all scores.\n",
    "\n",
    "Return **only** the final JSON object with keys \"scores\" and \"overall_score\".\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    # 2) Run the LLM three times and parse each JSON output\n",
    "    runs = []\n",
    "    for _ in range(3):\n",
    "        chain = LLMChain(llm=llm, prompt=eval_prompt)\n",
    "        raw = chain.run(\n",
    "            rubric=rubric_str,\n",
    "            question=question,\n",
    "            answer=answer\n",
    "        )\n",
    "        parsed = extract_json(raw)\n",
    "        runs.append(parsed)\n",
    "\n",
    "    # 3) Combine the three runs:\n",
    "    #    - Average numeric scores per criterion\n",
    "    #    - Collect the three explanations per criterion\n",
    "    first_scores = runs[0][\"scores\"]\n",
    "    combined_scores = []\n",
    "\n",
    "    for crit_obj in first_scores:\n",
    "        crit_name = crit_obj[\"name\"]\n",
    "\n",
    "        # Collect the three scores and explanations for this criterion (matching by name)\n",
    "        score_values = []\n",
    "        explanations = []\n",
    "\n",
    "        for run in runs:\n",
    "            match = next(item for item in run[\"scores\"] if item[\"name\"] == crit_name)\n",
    "            score_values.append(match[\"score\"])\n",
    "            explanations.append(match[\"explanation\"])\n",
    "\n",
    "        # 3a) Average the numeric scores\n",
    "        avg_score = round(sum(score_values) / len(score_values), 2)\n",
    "\n",
    "        # 3b) Summarize the three one‐sentence rationales into one concise sentence\n",
    "        summary_prompt = PromptTemplate(\n",
    "            input_variables=[\"exp0\", \"exp1\", \"exp2\"],\n",
    "            template=\"\"\"\n",
    "You have three one‐sentence rationales for the same evaluation criterion:\n",
    "1) {exp0}\n",
    "2) {exp1}\n",
    "3) {exp2}\n",
    "\n",
    "Please write a single, concise one‐sentence explanation that captures the essence of all three rationales.\n",
    "Return **only** that one‐sentence summary.\n",
    "\"\"\",\n",
    "        )\n",
    "        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "        raw_summary = summary_chain.run(\n",
    "            exp0=explanations[0],\n",
    "            exp1=explanations[1],\n",
    "            exp2=explanations[2]\n",
    "        )\n",
    "        summarized_explanation = raw_summary.strip()\n",
    "\n",
    "        combined_scores.append({\n",
    "            \"name\": crit_name,\n",
    "            \"score\": avg_score,\n",
    "            \"explanation\": summarized_explanation\n",
    "        })\n",
    "\n",
    "    # 4) Compute overall_score as the average of averaged criterion scores\n",
    "    if combined_scores:\n",
    "        overall = round(\n",
    "            sum(item[\"score\"] for item in combined_scores) / len(combined_scores), 2\n",
    "        )\n",
    "    else:\n",
    "        overall = 0.0\n",
    "\n",
    "    return {\n",
    "        \"scores\": combined_scores,\n",
    "        \"overall_score\": overall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba61b98",
   "metadata": {},
   "source": [
    "## 4. Load Rubrics & Old Scores JSONs\n",
    "\n",
    "- **Technical**  \n",
    "  - `tech_rubric.json`: JSON array of 5–8 technical evaluation criteria.  \n",
    "  - `evaluation_results.json`: Pre‐computed LLM‐evaluated results for `dataset.csv`.  \n",
    "- **HR**  \n",
    "  - `Data/HR/hr_rubric.json`: JSON array of 5–8 HR evaluation criteria.  \n",
    "  - `Data/HR/hr_evaluation_results_with_samples.json`: Pre‐computed HR evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41762a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Technical rubric & old results\n",
    "TECH_RUBRIC_PATH      = \"Data/Technical/tech_rubric.json\"\n",
    "TECH_OLD_RESULTS_PATH = \"Data/Technical/tech_evaluation_results.json\"\n",
    "\n",
    "tech_rubric = json.load(open(TECH_RUBRIC_PATH, \"r\", encoding=\"utf-8\"))\n",
    "tech_old    = json.load(open(TECH_OLD_RESULTS_PATH, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Build question → old overall_score map for Technical\n",
    "tech_past_scores = {\n",
    "    entry[\"question\"]: entry[\"evaluation\"].get(\"overall_score\", 0.0)\n",
    "    for entry in tech_old\n",
    "}\n",
    "\n",
    "# 4.2 HR rubric & old results\n",
    "HR_RUBRIC_PATH      = \"Data\\HR\\hr_rubric.json\"\n",
    "HR_OLD_RESULTS_PATH = \"Data\\HR\\hr_evaluation_results_with_samples.json\"\n",
    "\n",
    "hr_rubric = json.load(open(HR_RUBRIC_PATH, \"r\", encoding=\"utf-8\"))\n",
    "hr_old    = json.load(open(HR_OLD_RESULTS_PATH, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Build question → old overall_score map for HR\n",
    "hr_past_scores = {\n",
    "    entry[\"question\"]: entry[\"evaluation\"].get(\"overall_score\", 0.0)\n",
    "    for entry in hr_old\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf70b4",
   "metadata": {},
   "source": [
    "## 5. Load Datasets & Build FAISS Indexes\n",
    "\n",
    "- **Technical dataset**  \n",
    "  - `dataset.csv` should contain columns `\"question\",\"answer\"`.  \n",
    "  - We build a FAISS index on `question` texts for exact‐match/relevance checks.\n",
    "\n",
    "- **HR dataset**  \n",
    "  - `Data/HR/interview_best_answers_samples.csv` should contain `\"question\",\"answer\"`.  \n",
    "  - We similarly build a FAISS index on HR questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f844ff8",
   "metadata": {},
   "source": [
    "### 5.1 Technical dataset & FAISS index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074500ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Technical dataset & FAISS index\n",
    "TECH_CSV_PATH = \"Data\\Technical\\dataset.csv\"  # contains \"question\",\"answer\"\n",
    "df_tech = pd.read_csv(TECH_CSV_PATH).dropna(subset=[\"question\", \"answer\"])\n",
    "tech_questions = df_tech[\"question\"].astype(str).tolist()\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "tech_vectorstore = FAISS.from_texts(tech_questions, embeddings)\n",
    "tech_retriever   = tech_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 5.2 HR dataset & FAISS index\n",
    "HR_CSV_PATH = \"Data\\HR\\interview_best_answers_samples.csv\"\n",
    "df_hr = pd.read_csv(HR_CSV_PATH).dropna(subset=[\"question\", \"answer\"])\n",
    "hr_questions = df_hr[\"question\"].astype(str).tolist()\n",
    "\n",
    "hr_vectorstore = FAISS.from_texts(hr_questions, embeddings)\n",
    "hr_retriever   = hr_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3c243",
   "metadata": {},
   "source": [
    "## 6. Set Up EXACT‐MATCH and RELEVANCE Chains\n",
    "\n",
    "For both Technical and HR, we create:\n",
    "1. An **exact‐match** `RetrievalQA` chain to see if the new question exactly matches one in our dataset.  \n",
    "2. A **relevance** `LLMChain` that asks the model, “Of these top‐3 retrieved, which are truly relevant?” and returns a JSON array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a4940",
   "metadata": {},
   "source": [
    "### 6.1 Technical EXACT‐MATCH chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fc4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Technical EXACT‐MATCH chain\n",
    "tech_match_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that decides if a new Technical question exactly matches one in the dataset.\n",
    "From these retrieved questions:\n",
    "{context}\n",
    "\n",
    "New Question:\n",
    "{question}\n",
    "\n",
    "Respond with **exactly**:\n",
    "- YES: \"<matched question>\"\n",
    "- NO\n",
    "\"\"\",\n",
    ")\n",
    "tech_match_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=tech_retriever,\n",
    "    chain_type_kwargs={\"prompt\": tech_match_prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed79725",
   "metadata": {},
   "source": [
    "### 6.2 Technical RELEVANCE chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07686498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koty9\\AppData\\Local\\Temp\\ipykernel_31188\\3255714431.py:16: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  tech_relevance_chain = LLMChain(llm=llm, prompt=tech_relevance_prompt)\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Technical RELEVANCE chain\n",
    "tech_relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_question\", \"candidates\"],\n",
    "    template=\"\"\"\n",
    "New question:\n",
    "{new_question}\n",
    "\n",
    "Here are 3 candidate questions retrieved from the Technical dataset:\n",
    "{candidates}\n",
    "\n",
    "For each candidate, respond with YES or NO if it's truly relevant to the new question.\n",
    "Return a JSON array with only those candidate strings that are relevant.\n",
    "Example: [\"Q1 text\",\"Q3 text\"]\n",
    "\"\"\",\n",
    ")\n",
    "tech_relevance_chain = LLMChain(llm=llm, prompt=tech_relevance_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f6abc",
   "metadata": {},
   "source": [
    "### 6.3 HR EXACT‐MATCH chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0fd0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 HR EXACT‐MATCH chain\n",
    "hr_match_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that decides if a new HR question exactly matches one in the HR dataset.\n",
    "From these retrieved questions:\n",
    "{context}\n",
    "\n",
    "New Question:\n",
    "{question}\n",
    "\n",
    "Respond with **exactly**:\n",
    "- YES: \"<matched question>\"\n",
    "- NO\n",
    "\"\"\",\n",
    ")\n",
    "hr_match_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=hr_retriever,\n",
    "    chain_type_kwargs={\"prompt\": hr_match_prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cbaca",
   "metadata": {},
   "source": [
    "### 6.4 HR RELEVANCE chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947099ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6.4 HR RELEVANCE chain\n",
    "hr_relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_question\", \"candidates\"],\n",
    "    template=\"\"\"\n",
    "New question:\n",
    "{new_question}\n",
    "\n",
    "Here are 3 candidate questions retrieved from the HR dataset:\n",
    "{candidates}\n",
    "\n",
    "For each candidate, respond with YES or NO if it's truly relevant to the new question.\n",
    "Return a JSON array with only those relevant candidate strings.\n",
    "Example: [\"Tell me about yourself.\",\"What are your strengths?\"]\n",
    "\"\"\",\n",
    ")\n",
    "hr_relevance_chain = LLMChain(llm=llm, prompt=hr_relevance_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8fbeb",
   "metadata": {},
   "source": [
    "## 7. Master List of Questions (4 Technical + 4 HR)\n",
    "\n",
    "We expand from six questions to eight: four Technical and four HR. Each entry has:\n",
    "- `\"question\"`: the exact question text.\n",
    "- `\"type\"`: either `\"Technical\"` or `\"HR\"`.\n",
    "\n",
    "Feel free to swap these out for your actual eight interview questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17f5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_QUESTIONS = [\n",
    "    # --- Four Technical questions ---\n",
    "    {\n",
    "        \"question\": \"Explain how you would implement a Transformer-based Speech Emotion Recognition (SER) pipeline end-to-end, from raw audio input to predicted emotion labels.\",\n",
    "        \"type\": \"Technical\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Given a large dataset of audio recordings, describe at least three feature-extraction techniques (e.g., Mel-spectrogram, log-Mel, MFCC) and discuss the pros and cons of each for a SER task.\",\n",
    "        \"type\": \"Technical\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does Retrieval-Augmented Generation (RAG) improve answer evaluation compared to using a vanilla LLM? Illustrate with a pseudo-code or high-level workflow.\",\n",
    "        \"type\": \"Technical\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Suppose you have to fine-tune a pre-trained wav2vec 2.0 model on a new emotional‐speech dataset. Which steps would you follow (data preprocessing, training loop, hyperparameter tuning), and why?\",\n",
    "        \"type\": \"Technical\"\n",
    "    },\n",
    "    # --- Four HR questions ---\n",
    "    {\n",
    "        \"question\": \"Tell me about a time you faced a conflict in a team. How did you resolve it?\",\n",
    "        \"type\": \"HR\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are your greatest strengths and how do they apply to this role?\",\n",
    "        \"type\": \"HR\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Describe a situation when you had to adapt quickly to a significant change at work or in school. What did you learn?\",\n",
    "        \"type\": \"HR\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you handle constructive criticism? Give an example.\",\n",
    "        \"type\": \"HR\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Build a lookup map: question_text → type\n",
    "QUESTION_TYPE_MAP = { entry[\"question\"]: entry[\"type\"] for entry in MASTER_QUESTIONS }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1039dde",
   "metadata": {},
   "source": [
    "## 8. Unified Evaluation Function\n",
    "\n",
    "`evaluate_question_answer(...)` takes a `(question, answer)` pair, looks up whether it is Technical or HR, then applies:\n",
    "\n",
    "1. **Exact‐Match Check** (using the relevant FAISS retriever + `RetrievalQA`):\n",
    "   - If the model returns `YES: \"<matched question>\"` and the old score > 70, combine 70% old + 30% fresh rubric.\n",
    "   - If old score ≤ 70, use 100% fresh rubric.\n",
    "2. **Relevance Check** (if no exact match): \n",
    "   - Get the top‐3 neighbors from FAISS, ask “which are truly relevant?” via an LLM chain.\n",
    "   - If none relevant, use 100% fresh rubric.\n",
    "   - If some relevant, average their old scores, then combine 30% average + 70% fresh rubric.\n",
    "\n",
    "Returns a dictionary with:\n",
    "- `\"question\"`, `\"type\"`,  \n",
    "- `\"old_dataset_score\"`, `\"rubric_score\"`, `\"final_combined_score\"`,  \n",
    "- `\"rubric_breakdown\"` (detailed per‐criterion scores + explanations),  \n",
    "- Optionally `\"error\"` if something fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a3b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_instructional(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detects if a given HR answer is actually meta-instructions (e.g., “Best strategy: …”).\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"^\\s*(Start with|Remember that|BEST ANSWERS?|Best strategy|Example:|Remember, you|To answer this question|If you want to|The only right answer|To cover both|Many executives)\",\n",
    "        r\"\\b(you should|you must|always|never|exercise)\\b\"\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        if re.search(pat, text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_question_answer(question: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Version A: Pre-check “I don’t know” (or very short answers).\n",
    "    If answer is literally “I don’t know” (case-insensitive) or fewer than 3 words,\n",
    "    return zero scores immediately. Otherwise, proceed with exact/relevance logic.\n",
    "    \"\"\"\n",
    "    q_type = QUESTION_TYPE_MAP.get(question)\n",
    "    if q_type not in (\"Technical\", \"HR\"):\n",
    "        raise ValueError(f\"Question not found in MASTER_QUESTIONS: {question}\")\n",
    "\n",
    "    # Identify which rubric and chains to use\n",
    "    if q_type == \"Technical\":\n",
    "        old_scores_map  = tech_past_scores\n",
    "        old_df          = df_tech\n",
    "        old_match_chain = tech_match_chain\n",
    "        old_relev_chain = tech_relevance_chain\n",
    "        retriever       = tech_retriever\n",
    "        rubric          = tech_rubric\n",
    "    else:  # HR\n",
    "        old_scores_map  = hr_past_scores\n",
    "        old_df          = df_hr\n",
    "        old_match_chain = hr_match_chain\n",
    "        old_relev_chain = hr_relevance_chain\n",
    "        retriever       = hr_retriever\n",
    "        rubric          = hr_rubric\n",
    "\n",
    "    # 1) PRE-CHECK: if answer is “I don't know” / “idk” / extremely short, return zeros\n",
    "    normalized = answer.strip().lower()\n",
    "    if normalized in [\"i don’t know\", \"i don't know\", \"idk\", \"no idea\"] or len(normalized.split()) < 3:\n",
    "        zero_breakdown = [\n",
    "            {\n",
    "                \"name\": crit[\"name\"],\n",
    "                \"score\": 0.0,\n",
    "                \"explanation\": \"No substantive answer provided.\"\n",
    "            }\n",
    "            for crit in rubric\n",
    "        ]\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"type\": q_type,\n",
    "            \"old_dataset_score\": 0.0,\n",
    "            \"rubric_score\": 0.0,\n",
    "            \"final_combined_score\": 0.0,\n",
    "            \"rubric_breakdown\": {\n",
    "                \"scores\": zero_breakdown,\n",
    "                \"overall_score\": 0.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # 2) If not “I don't know,” proceed with EXACT-MATCH / RELEVANCE logic as before:\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"type\": q_type,\n",
    "        \"old_dataset_score\": 0.0,\n",
    "        \"rubric_score\": 0.0,\n",
    "        \"final_combined_score\": 0.0,\n",
    "        \"rubric_breakdown\": None,\n",
    "    }\n",
    "\n",
    "    # 2a) Retrieve top-3 neighbors\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    # 2b) Exact-match check\n",
    "    match_out = old_match_chain.run(question).strip()\n",
    "    if match_out.upper().startswith(\"YES\"):\n",
    "        m = re.search(r'YES:\\s*\"(.*)\"', match_out)\n",
    "        exact_q = m.group(1) if m else None\n",
    "        old_score = old_scores_map.get(exact_q, 0.0)\n",
    "        result[\"old_dataset_score\"] = old_score\n",
    "\n",
    "        if old_score > 70:\n",
    "            # Combine 70% old + 30% fresh rubric\n",
    "            rub_report = evaluate_with_rubric(question, answer, rubric)\n",
    "            rub_score  = rub_report[\"overall_score\"]\n",
    "            result[\"rubric_score\"]         = rub_score\n",
    "            result[\"final_combined_score\"] = round(0.7 * old_score + 0.3 * rub_score, 2)\n",
    "            result[\"rubric_breakdown\"]     = rub_report\n",
    "        else:\n",
    "            # old_score ≤ 70 → 100% fresh rubric\n",
    "            rub_report = evaluate_with_rubric(question, answer, rubric)\n",
    "            rub_score  = rub_report[\"overall_score\"]\n",
    "            result[\"rubric_score\"]         = rub_score\n",
    "            result[\"final_combined_score\"] = rub_score\n",
    "            result[\"rubric_breakdown\"]     = rub_report\n",
    "\n",
    "        return result\n",
    "\n",
    "    # 2c) No exact match → RELEVANCE check\n",
    "    candidates_json = json.dumps([d.page_content for d in docs], indent=2)\n",
    "    rel_raw = old_relev_chain.run(new_question=question, candidates=candidates_json)\n",
    "\n",
    "    try:\n",
    "        relevant_list = extract_json(rel_raw)\n",
    "    except ValueError:\n",
    "        relevant_list = []\n",
    "\n",
    "    if not relevant_list:\n",
    "        # 2c.i) No relevant → 100% fresh rubric\n",
    "        rub_report = evaluate_with_rubric(question, answer, rubric)\n",
    "        rub_score  = rub_report[\"overall_score\"]\n",
    "        result[\"rubric_score\"]         = rub_score\n",
    "        result[\"final_combined_score\"] = rub_score\n",
    "        result[\"rubric_breakdown\"]     = rub_report\n",
    "        return result\n",
    "    else:\n",
    "        # 2c.ii) Some relevant → average their old overall_scores\n",
    "        old_scores_accum = []\n",
    "        for q_old in relevant_list:\n",
    "            try:\n",
    "                a_old = old_df.loc[old_df.question == q_old, \"answer\"].iloc[0]\n",
    "            except IndexError:\n",
    "                a_old = \"\"\n",
    "\n",
    "            if q_type == \"HR\" and is_instructional(a_old):\n",
    "                try:\n",
    "                    a_old = convert_to_sample_answer(q_old, a_old, llm)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            old_rub_report = evaluate_with_rubric(q_old, a_old, rubric)\n",
    "            old_scores_accum.append(old_rub_report[\"overall_score\"])\n",
    "\n",
    "        avg_old = sum(old_scores_accum) / len(old_scores_accum)\n",
    "        result[\"old_dataset_score\"] = avg_old\n",
    "\n",
    "        # Fresh rubric on new (question, answer)\n",
    "        new_rub_report = evaluate_with_rubric(question, answer, rubric)\n",
    "        rub_score      = new_rub_report[\"overall_score\"]\n",
    "        result[\"rubric_score\"]       = rub_score\n",
    "        result[\"rubric_breakdown\"]   = new_rub_report\n",
    "\n",
    "        combined = round(0.7 * rub_score + 0.3 * avg_old, 2)\n",
    "        result[\"final_combined_score\"] = combined\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e3439",
   "metadata": {},
   "source": [
    "## 9. Main Interactive Loop\n",
    "\n",
    "Iterate over the eight questions in `MASTER_QUESTIONS`. For each:\n",
    "1. Print the question and its type (Technical/HR).\n",
    "2. Prompt the user to paste the candidate’s answer.\n",
    "3. Call `evaluate_question_answer(...)`.\n",
    "4. Print the JSON‐formatted result (per‐criterion breakdown + combined score).\n",
    "5. Save all eight results to `candidate_evaluation_summary.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f417c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Candidate Evaluation Script (3 Random Questions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**QUESTION (Technical):**  \n",
       "Given a large dataset of audio recordings, describe at least three feature-extraction techniques (e.g., Mel-spectrogram, log-Mel, MFCC) and discuss the pros and cons of each for a SER task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Candidate’s answer:**\n",
      "no kholedge\n",
      "\n",
      "**Best retrieved question from dataset:**\n",
      "Which feature selection techniques do you know?\n",
      "\n",
      "Evaluating… Done.\n",
      "\n",
      "{\n",
      "  \"question\": \"Given a large dataset of audio recordings, describe at least three feature-extraction techniques (e.g., Mel-spectrogram, log-Mel, MFCC) and discuss the pros and cons of each for a SER task.\",\n",
      "  \"type\": \"Technical\",\n",
      "  \"old_dataset_score\": 0.0,\n",
      "  \"rubric_score\": 0.0,\n",
      "  \"final_combined_score\": 0.0,\n",
      "  \"rubric_breakdown\": {\n",
      "    \"scores\": [\n",
      "      {\n",
      "        \"name\": \"Clarity\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Accuracy\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Completeness\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Relevance\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Depth\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Conciseness\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Engagement\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_score\": 0.0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**QUESTION (Technical):**  \n",
       "Explain how you would implement a Transformer-based Speech Emotion Recognition (SER) pipeline end-to-end, from raw audio input to predicted emotion labels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Candidate’s answer:**\n",
      "using cnn\n",
      "\n",
      "**Best retrieved question from dataset:**\n",
      "How can we use machine learning for text classification?\n",
      "\n",
      "Evaluating… Done.\n",
      "\n",
      "{\n",
      "  \"question\": \"Explain how you would implement a Transformer-based Speech Emotion Recognition (SER) pipeline end-to-end, from raw audio input to predicted emotion labels.\",\n",
      "  \"type\": \"Technical\",\n",
      "  \"old_dataset_score\": 0.0,\n",
      "  \"rubric_score\": 0.0,\n",
      "  \"final_combined_score\": 0.0,\n",
      "  \"rubric_breakdown\": {\n",
      "    \"scores\": [\n",
      "      {\n",
      "        \"name\": \"Clarity\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Accuracy\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Completeness\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Relevance\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Depth\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Conciseness\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Engagement\",\n",
      "        \"score\": 0.0,\n",
      "        \"explanation\": \"No substantive answer provided.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_score\": 0.0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**QUESTION (HR):**  \n",
       "Tell me about a time you faced a conflict in a team. How did you resolve it?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Candidate’s answer:**\n",
      "i communicated with them\n",
      "\n",
      "**Best retrieved question from dataset:**\n",
      "Tell me about a situation when your work was criticized.\n",
      "\n",
      "Evaluating… Done.\n",
      "\n",
      "{\n",
      "  \"question\": \"Tell me about a time you faced a conflict in a team. How did you resolve it?\",\n",
      "  \"type\": \"HR\",\n",
      "  \"old_dataset_score\": 0.0,\n",
      "  \"rubric_score\": 37.62,\n",
      "  \"final_combined_score\": 37.62,\n",
      "  \"rubric_breakdown\": {\n",
      "    \"scores\": [\n",
      "      {\n",
      "        \"name\": \"Relevance\",\n",
      "        \"score\": 50.0,\n",
      "        \"explanation\": \"The answer is relevant to resolving team conflict through communication but lacks specific detail.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Clarity\",\n",
      "        \"score\": 36.67,\n",
      "        \"explanation\": \"The response is unclear, excessively brief, and lacks sufficient detail to effectively convey the situation, communication process, or resolution.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Professionalism\",\n",
      "        \"score\": 53.33,\n",
      "        \"explanation\": \"The tone is neutral and professional but lacks the depth and context typically expected in HR or professional settings.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Depth of Insight\",\n",
      "        \"score\": 20.0,\n",
      "        \"explanation\": \"The answer lacks substantial insights or examples regarding the process and outcome of handling team conflict resolution.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Positivity\",\n",
      "        \"score\": 50.0,\n",
      "        \"explanation\": \"The response maintains a neutral tone, avoiding positivity or negativity in its framing.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Engagement\",\n",
      "        \"score\": 26.67,\n",
      "        \"explanation\": \"The answer fails to invite or encourage further discussion or engagement due to its lack of detail.\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Adaptability\",\n",
      "        \"score\": 26.67,\n",
      "        \"explanation\": \"The response lacks adaptability and fails to connect experiences or apply skills effectively to the role or conflict resolution.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_score\": 37.62\n",
      "  }\n",
      "}\n",
      "\n",
      "All results written to candidate_evaluation_summary.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 9. Main Interactive Loop (Jupyter‐friendly prints + flush) ─────────────────────────────────────────────────────\n",
    "import random\n",
    "import sys\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Randomly select 2 Technical and 1 HR question from MASTER_QUESTIONS,\n",
    "    then ask the user to paste the candidate’s answer for each.\n",
    "    Show:\n",
    "      - The selected question itself (as Markdown)\n",
    "      - The candidate's answer\n",
    "      - The best‐retrieved question (top‐1 from FAISS)\n",
    "    Then call evaluate_question_answer(...) and print its JSON output.\n",
    "    Finally, write all results to candidate_evaluation_summary.json.\n",
    "    \"\"\"\n",
    "    display(Markdown(\"## Candidate Evaluation Script (3 Random Questions)\"))\n",
    "\n",
    "    # 1) Build pools and randomly pick\n",
    "    tech_pool = [q[\"question\"] for q in MASTER_QUESTIONS if q[\"type\"] == \"Technical\"]\n",
    "    hr_pool   = [q[\"question\"] for q in MASTER_QUESTIONS if q[\"type\"] == \"HR\"]\n",
    "\n",
    "    selected_tech = random.sample(tech_pool, k=2)\n",
    "    selected_hr   = random.sample(hr_pool, k=1)\n",
    "    selected_questions = selected_tech + selected_hr\n",
    "    random.shuffle(selected_questions)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for q_text in selected_questions:\n",
    "        q_type = QUESTION_TYPE_MAP[q_text]\n",
    "\n",
    "        # 2) Display the question itself using Markdown\n",
    "        display(Markdown(f\"**QUESTION ({q_type}):**  \\n{q_text}\"))\n",
    "\n",
    "        # 3) Prompt for candidate's answer (question remains visible above)\n",
    "        prompt_str = f\"Enter candidate’s answer for the above question:\\n> \"\n",
    "        user_ans = input(prompt_str).strip()\n",
    "\n",
    "        # 4) Immediately echo candidate's answer (print will show under the prompt)\n",
    "        print(f\"\\n**Candidate’s answer:**\\n{user_ans}\\n\")\n",
    "\n",
    "        # 5) Retrieve and display the best FAISS neighbor (top‐1)\n",
    "        retriever = tech_retriever if q_type == \"Technical\" else hr_retriever\n",
    "        docs = retriever.get_relevant_documents(q_text)\n",
    "        if docs:\n",
    "            best_q = docs[0].page_content\n",
    "            print(f\"**Best retrieved question from dataset:**\\n{best_q}\\n\")\n",
    "        else:\n",
    "            print(\"**No retrieved questions found.**\\n\")\n",
    "\n",
    "        # 6) Print “Evaluating…” and flush stdout immediately\n",
    "        print(\"Evaluating…\", end=\" \", flush=True)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        try:\n",
    "            report = evaluate_question_answer(q_text, user_ans)\n",
    "\n",
    "            # Once done, print “Done.” and full JSON\n",
    "            print(\"Done.\\n\")\n",
    "            print(json.dumps(report, indent=2))\n",
    "            all_results.append(report)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print any error, then continue\n",
    "            print(\"ERROR:\", e)\n",
    "            all_results.append({\n",
    "                \"question\": q_text,\n",
    "                \"type\": q_type,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    # 7) Write all results to disk\n",
    "    out_path = \"candidate_evaluation_summary.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nAll results written to {out_path}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe18d3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- We have now separated the code into distinct notebook cells with explanatory Markdown.  \n",
    "- File paths (`<PATH_TO_YOUR>/…`) are taken directly from your provided template.  \n",
    "- `MASTER_QUESTIONS` has been expanded from 6 to 8 (4 Technical + 4 HR).  \n",
    "\n",
    "To use this notebook:\n",
    "1. Replace all `<PATH_TO_YOUR>` placeholders with your actual local file paths.  \n",
    "2. Replace the eight sample questions with the exact ones you wish to ask (maintaining `\"type\": \"Technical\"` or `\"type\": \"HR\"`).  \n",
    "3. Run each cell in order, then execute the `main()` cell.  \n",
    "4. For each question, paste the candidate’s answer when prompted.  \n",
    "5. At the end, review `candidate_evaluation_summary.json`, which contains per‐question breakdowns and overall combined scores.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
